# logstash-indexer.conf
input { 
  redis { 
    host => "127.0.0.1" 
    data_type => "list" 
    type => "redis" 
    key => "logstash" 
    message_format => "json_event"
    debug => true
  } 
}

filter {
  # Check if syslog message has PRI using grep.   If so then :
  #   strip the syslog PRI part and create facility and severity fields.
  #   the original syslog message is saved in field %{syslog_raw_message}.
  #   the extracted PRI is available in the %{syslog_pri} field.
  #
  #   You get %{syslog_facility_code} and %{syslog_severity_code} fields.
  #   You also get %{syslog_facility} and %{syslog_severity} fields if the
  #   use_labels option is set True (the default) on syslog_pri filter.
 grok {
      type => "syslog"
      pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:source_timestamp} %{DATE:date} %{HOUR:hour}: %{MINUTE:minute}:%{SECOND:second} %{SYSLOGHOST:syslog_hostname} %{LOGLEVEL:logging_level} %{WORD:tenant} %{GREEDYDATA:syslog_message}" ]
##      pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:source_timestamp} %{DATE:date} %{HOUR:hr}: %{MINUTE:min}:%{SECOND:sec},%{POSINT:ms} %{SYSLOGHOST:syslog_hostname} %{LOGLEVEL:logging_level} %{WORD:tenant} %{GREEDYDATA:syslog_message}" ]
##      pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{LOGLEVEL:logging_level} %{WORD:tenant} %{GREEDYDATA:syslog_message}" ]
##      pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{LOGLEVEL:logging_level} %{WORD:tenant} %{GREEDYDATA:syslog_message}" ]
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{@source_host}" ]
      add_tag => [ "%{source_timestamp}", "%{syslog_hostname}", "%{logging_level}", "%{tenant}" ]
  }
  syslog_pri {
      type => "syslog"
  }
  date {
      type => "syslog"
      match => [ "source_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
  } 
  mutate {
      type => "syslog"
      exclude_tags => "_grokparsefailure"
      replace => [ "@source_host", "%{syslog_hostname}" ]
      replace => [ "@message", "%{syslog_message}" ]
  }
  mutate {
      type => "syslog"
      remove => [ "syslog_hostname", "syslog_message", "source_timestamp" ]
##    remove => [ "syslog_hostname", "source_timestamp" ]
  }

 grep {
	type => "syslog"
	match => [ "@message", "network_id" ]
	add_tag => "has_n/w_id"
	drop => false
	}
 grok {
	type => "syslog"
	tags => "has_n/w_id"
	pattern => [ "%{WORD:network_name}:%{UUID:netuuid} %{GREEDYDATA:message_remainder}" ]
	remove_tag => [ "has_n/w_id" ]
	add_tag =>  [ "net_id:%{netuuid}" , "test"]
       }	
 mutate {
	type => "syslog" 
	tags => [ "test" ]
	replace => [ "@message", "%{message_remainder}" ]
	remove => [ "message_remainder" ]
	remove_tag => [ "test" ]
	}
}
output {
  stdout {
	debug => true
	debug_format => "json"
 }
  elasticsearch {
#bind_host => "192.168.0.130"
       type => "syslog"
#cluster => "oc"
#node_name => "oc-cloud1"
	host => "192.168.100.221"
        # Uncomment below if you wish syslog messages to have their own ES index.
        # index => "logstash-syslog-%{+YYYY.MM.dd}"
  }
}
